{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## After training the model, we can use this notebook to test the model. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Direct download the model and test. Run this notebook in A100 GPU machine (NC24adsA100 compute instance)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning availability is subject to regional constraints. The Llama-2-70b, Llama-2-7b, and Llama-2-13b models are exclusive to projects based in **WestUS3**. You will need to select a project in one of these regions or create a new project in a supported region to being fine-tuning.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "import time\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "credential = DefaultAzureCredential()\n",
        "subscription_id = \"8480def5-8f7a-4285-99f7-295b61d7b22a\" # your subscription id\n",
        "resource_group = \"louisli-rg\"#your resource group\n",
        "workspace = \"training\" #your workspace name\n",
        "workspace_ml_client = MLClient(credential, subscription_id, resource_group, workspace)\n",
        "\n",
        "model_name = \"llama2_13b_chat_sql_tuned\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1709828809426
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For regular model "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for m in workspace_ml_client.models.list(model_name):\n",
        "    print(m)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "creation_context:\n  created_at: '2024-03-06T19:18:23.438433+00:00'\n  created_by: Louis Li (AI)\n  created_by_type: User\n  last_modified_at: '2024-03-06T19:18:23.438433+00:00'\n  last_modified_by: Louis Li (AI)\n  last_modified_by_type: User\nflavors:\n  python_function:\n    artifacts: \"{\\n  \\\"pipeline\\\": {\\n    \\\"path\\\": \\\"artifacts/trained_model\\\",\\n\\\n      \\    \\\"uri\\\": \\\"trained_model\\\"\\n  }\\n}\"\n    cloudpickle_version: 3.0.0\n    env: \"{\\n  \\\"conda\\\": \\\"conda.yaml\\\",\\n  \\\"virtualenv\\\": \\\"python_env.yaml\\\"\\n\\\n      }\"\n    loader_module: mlflow.pyfunc.model\n    python_model: python_model.pkl\n    python_version: 3.9.16\nid: azureml:/subscriptions/8480def5-8f7a-4285-99f7-295b61d7b22a/resourceGroups/louisli-rg/providers/Microsoft.MachineLearningServices/workspaces/training/models/llama2_13b_chat_sql_tuned/versions/1\njob_name: 098b87c6-6f51-4ddb-a220-6859e290d6cd\nname: llama2_13b_chat_sql_tuned\npath: azureml://subscriptions/8480def5-8f7a-4285-99f7-295b61d7b22a/resourceGroups/louisli-rg/workspaces/training/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned\nproperties:\n  azureml.artifactPrefix: ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned\n  azureml.storagePath: ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned\n  flavors: python_function\n  flavors.python_function: \"{\\n  \\\"cloudpickle_version\\\": \\\"3.0.0\\\",\\n  \\\"python_model\\\"\\\n    : \\\"python_model.pkl\\\",\\n  \\\"artifacts\\\": {\\n    \\\"pipeline\\\": {\\n      \\\"path\\\"\\\n    : \\\"artifacts/trained_model\\\",\\n      \\\"uri\\\": \\\"trained_model\\\"\\n    }\\n  },\\n\\\n    \\  \\\"loader_module\\\": \\\"mlflow.pyfunc.model\\\",\\n  \\\"python_version\\\": \\\"3.9.16\\\"\\\n    ,\\n  \\\"env\\\": {\\n    \\\"conda\\\": \\\"conda.yaml\\\",\\n    \\\"virtualenv\\\": \\\"python_env.yaml\\\"\\\n    \\n  }\\n}\"\n  mlflow.modelSourceUri: azureml://swedencentral.api.azureml.ms/mlflow/v2.0/subscriptions/8480def5-8f7a-4285-99f7-295b61d7b22a/resourceGroups/louisli-rg/providers/Microsoft.MachineLearningServices/workspaces/training/experiments/b0d14228-a433-4376-bc1a-0d630b34272d/runs/098b87c6-6f51-4ddb-a220-6859e290d6cd/artifacts/llama2_13b_chat_sql_tuned\n  model_json: '{\"run_id\": \"098b87c6-6f51-4ddb-a220-6859e290d6cd\", \"artifact_path\":\n    \"llama2_13b_chat_sql_tuned\", \"utc_time_created\": \"2024-03-06 19:14:32.865800\",\n    \"flavors\": {\"python_function\": {\"cloudpickle_version\": \"3.0.0\", \"python_model\":\n    \"python_model.pkl\", \"artifacts\": {\"pipeline\": {\"path\": \"artifacts/trained_model\",\n    \"uri\": \"trained_model\"}}, \"loader_module\": \"mlflow.pyfunc.model\", \"python_version\":\n    \"3.9.16\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}}, \"model_uuid\":\n    \"b8bad7a429bf424584f287b78599109d\", \"mlflow_version\": \"2.11.1\", \"model_size_bytes\":\n    26033765461}'\nstage: Development\ntags: {}\ntype: mlflow_model\nversion: '1'\n\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709828810811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path=\"./\"\n",
        "workspace_ml_client.models.download(model_name, version=\"1\",download_path=model_path)\n",
        "#after this step, remove the redundant parent folder name \"llama2_13b_fine_tuned\" so that the downloaded folder only has one "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Downloading the model ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned at .//llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned\n\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\nYour file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n\nExample: azcopy copy 'https://training5027750754.blob.core.windows.net/azureml/ExperimentRun/dcid.098b87c6-6f51-4ddb-a220-6859e290d6cd/llama2_13b_chat_sql_tuned' './/llama2_13b_chat_sql_tuned/llama2_13b_chat_sql_tuned' \n\nSee https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1709829174447
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "example = {\"context\":\"You are querying the sales database, what is the SQL query for the following question?\",\"input\":\"What is the total revenue for each territory?\"}\n",
        "PROMPT_DICT =\"\\n{context}\\n\\n### Question:\\n{input}\\n\\n### Response:{output}\"\n",
        "PROMPT_DICT_CHAT =\"<s>[INST]\\n{context}\\n\\n### Question:\\n{input}\\n[/INST]\"\n",
        "model = mlflow.pyfunc.load_model(model_name)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1709828018999
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For regular model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "prompt = PROMPT_DICT.format(input=example[\"input\"], context=example[\"context\"])\n",
        "prompt = {\"role\": \"user\",\"content\": prompt} \n",
        "model.predict([prompt])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Chat Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "prompt = PROMPT_DICT_CHAT.format(input=example[\"input\"], context=example[\"context\"])\n",
        "prompt = {\"role\": \"user\",\"content\": prompt} \n",
        "model.predict([prompt])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Deploy to managed online endpoint and test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create online endpoint: ```az ml online-endpoint create -f deployment/endpoint.yml```\n",
        "2. Create the deployment: ```az ml online-deployment update -f deployment/deployment.yml```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "def allowSelfSignedHttps(allowed):\n",
        "    # bypass the server certificate verification on client side\n",
        "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
        "\n",
        "# Request data goes here\n",
        "# The example below assumes JSON formatting which may be updated\n",
        "# depending on the format your endpoint expects.\n",
        "# More information can be found here:\n",
        "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "question = \"What is the average unit price of products by each supplier?\"\n",
        "\n",
        "# content = \"Hi there\"\n",
        "\n",
        "data= {\"data\":{\"text\":[question], \"max_gen_len\":100, \"temperature\":0.9}}\n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = 'https://llma2-fine-tuning.westus2.inference.ml.azure.com/score'\n",
        "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
        "api_key = ''\n",
        "if not api_key:\n",
        "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
        "# Remove this header to have the request observe the endpoint traffic rules\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(error.read().decode(\"utf8\", 'ignore'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "b'{\"output\": \"#\\\\n### Response:SELECT Suppliers.SupplierID, AVG(Products.UnitPrice) AS AverageUnitPrice FROM Products INNER JOIN Suppliers ON Products.SupplierID = Suppliers.SupplierID GROUP BY Suppliers.SupplierID\\\\n###\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"}'\n"
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Deploy to AKS and test"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#create AKS cluster\n",
        "az aks create -g ml -n aksgpu2 --enable-managed-identity --node-count 1 --enable-addons monitoring --generate-ssh-keys --node-vm-size standard_nc24ads_a100_v4\n",
        "\n",
        "#Install k8s-extension\n",
        "az k8s-extension create --name ml --extension-type Microsoft.AzureML.Kubernetes --config enableTraining=True enableInference=True inferenceRouterServiceType=LoadBalancer allowInsecureConnections=True InferenceRouterHA=False --cluster-type managedClusters --cluster-name aksgpu2 --resource-group ml --scope cluster\n",
        "\n",
        "#Install Nvidia extension\n",
        "az aks get-credentials --resource-group ml --name aksgpu2\n",
        "\n",
        "kubectl apply -f nvidia_device.yaml\n",
        "\n",
        "#create namespace\n",
        "kubectl create namespace gpu-resources\n",
        "\n",
        "#create instance type\n",
        "kubectl apply -f instance_type.yaml\n",
        "\n",
        "###az aks nodepool add --resource-group ml --cluster-name aks001 --name gpunp --node-count 1 --node-vm-size standard_nc24ads_a100_v4 --node-taints sku=gpu:NoSchedule --aks-custom-headers UseGPUDedicatedVHD=true --enable-cluster-autoscaler --min-count 1 --max-count 3\n",
        "\n",
        "#attach to azure ml workspace\n",
        "\n",
        "az ml compute attach --resource-group ml --workspace-name ws01ent --type Kubernetes --name aksgpu2 --resource-id \"/subscriptions/840b5c5c-3f4a-459a-94fc-6bad2a969f9d/resourcegroups/ml/providers/Microsoft.ContainerService/managedClusters/aksgpu2\" --identity-type SystemAssigned --no-wait --namespace gpu-resources\n",
        "\n",
        "#create the online endpoint\n",
        "az ml online-endpoint create -f k8s_endpoint.yml\n",
        "#create the deployment\n",
        "az ml online-deployment create -f k8s_deployment.yml\n",
        "\n",
        "\n",
        "\n",
        "#Delete deployments in case needed\n",
        "az ml online-deployment delete --name blue --endpoint-name llm-k8s-gpu --yes --resource-group ml --workspace-name ws01ent\n",
        "az ml online-deployment delete --name blue --endpoint-name llm-k8s-ep --yes --resource-group ml --workspace-name ws01ent\n",
        "az ml online-deployment delete --name green --endpoint-name llm-k8s-ep --yes --resource-group ml --workspace-name ws01ent\n",
        "\n",
        "az ml online-endpoint delete --name ws01ent-bajsw --resource-group ml --workspace-name ws01ent --yes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "def allowSelfSignedHttps(allowed):\n",
        "    # bypass the server certificate verification on client side\n",
        "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
        "\n",
        "# Request data goes here\n",
        "# The example below assumes JSON formatting which may be updated\n",
        "# depending on the format your endpoint expects.\n",
        "# More information can be found here:\n",
        "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "prompt = \"\"\"\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Summarize the following input to less than 30 words .\n",
        "### Input:\n",
        "In general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.\n",
        "A language model is a probability distribution over sentences: it’s both able to generate plausible human-written sentences (if it’s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be “perplexed” when presented with a well-written document.\n",
        "Thus, the perplexity metric in NLP is a way to capture the degree of ‘uncertainty’ a model has in predicting (i.e. assigning probabilities to) text.\"\"\"\n",
        "\n",
        "instruction =\"You are querying the sales database, what is the SQL query for the following input question?\"\n",
        "input = \"What is the average unit price of products by each supplier?\"\n",
        "content = f\"<s>[INST]\\n{instruction}\\n\\n### Input:\\n{input}\\n[/INST]\"\n",
        "\n",
        "# content = \"Hi there\"\n",
        "\n",
        "data= {\"data\":{\"text\":content, \"max_length\":100}}\n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = 'http://20.72.223.233/api/v1/endpoint/llm-k8s-gpu/score'\n",
        "api_key= ''\n",
        "if not api_key:\n",
        "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
        "# Remove this header to have the request observe the endpoint traffic rules\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(error.read().decode(\"utf8\", 'ignore'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}